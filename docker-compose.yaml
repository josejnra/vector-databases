# Windows/linux configuration

services:

  weaviate:
    container_name: weaviate
    command:
      - --host
      - 0.0.0.0
      - --port
      - '8080'
      - --scheme
      - http
    image: cr.weaviate.io/semitechnologies/weaviate
    ports:
      - 8080:8080
      - 50051:50051
    # volumes:
    # - ./weaviate_data:/var/lib/weaviate
    restart: on-failure:0
    environment:
      QUERY_DEFAULTS_LIMIT: 20
      AUTHENTICATION_ANONYMOUS_ACCESS_ENABLED: 'true'
      PERSISTENCE_DATA_PATH: '/var/lib/weaviate'
      DEFAULT_VECTORIZER_MODULE: text2vec-transformers
      ENABLE_MODULES: text2vec-transformers,text2vec-ollama,generative-ollama
      TRANSFORMERS_INFERENCE_API: http://t2v-transformers:8080
      CLUSTER_HOSTNAME: 'node1'

  t2v-transformers:
    image: cr.weaviate.io/semitechnologies/transformers-inference:sentence-transformers-multi-qa-MiniLM-L6-cos-v1
    container_name: t2v
    environment:
      ENABLE_CUDA: 0 # set to 1 to enable
      # NVIDIA_VISIBLE_DEVICES: all # enable if running with CUDA

  ollama:
    image: ollama/ollama
    container_name: ollama
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              # device_ids: ['0', '3']
              count: all
              capabilities: [gpu]
    environment:
      MODELS: all-minilm,mistral:latest 
    volumes:
      - ./ollama_data:/root/.ollama
    ports:
      - 11434:11434

  ollama-ui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: ollama-ui
    extra_hosts:
      - host.docker.internal:host-gateway
    environment:
      WEBUI_AUTH: false
      OLLAMA_BASE_URL: http://ollama:11434
      OLLAMA_API_BASE_URL: http://ollama:11434/api
    depends_on:
      - ollama
    ports:
      - 3000:8080
    # volumes:
    #   - ollama-ui-data:/app/backend/data
